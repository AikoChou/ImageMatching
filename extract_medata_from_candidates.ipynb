{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=500 #sampling here for the purpose of the user study\n",
    "rtl='rtl'\n",
    "ltr='ltr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=['enwiki','arwiki','kowiki','cswiki','viwiki','frwiki',\n",
    "           'fawiki','ptwiki','ruwiki','trwiki','plwiki','hewiki',\n",
    "           'svwiki','ukwiki','huwiki','hywiki','srwiki','euwiki',\n",
    "           'arzwiki','cebwiki','dewiki','bnwiki'] #language editions to consider\n",
    "rtls={\n",
    "    'enwiki':ltr,'arwiki':rtl,'kowiki':ltr,'cswiki':ltr,'viwiki':ltr,'frwiki':ltr,\n",
    "           'fawiki':rtl,'ptwiki':ltr,'ruwiki':ltr,'trwiki':ltr,'plwiki':ltr,'hewiki':rtl,\n",
    "           'svwiki':ltr,'ukwiki':ltr,'huwiki':ltr,'hywiki':ltr,'srwiki':ltr,'euwiki':ltr,\n",
    "           'arzwiki':rtl,'cebwiki':ltr,'dewiki':ltr,'bnwiki':ltr\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quering SQL replicas to get the list of images in each category through the categorylinks table (unavailable on Hive)\n",
    "\n",
    "**There must be a better way now that categorylinks is on hive**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_properties` works as follows:\n",
    "* First, it checks whether the image is actually on Commons, hence available for reuse - this is important as some images are only available for specific Wikipedias\n",
    "* It then extract information such as caption, description, categories and structure data. \n",
    "All this is done via the html page - it might make sense to improve the code below and integrate with existing methods that extract this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties(image_name,language,rtl):\n",
    "    #\n",
    "    #getting html page content or return 404 if not available\n",
    "    try:\n",
    "        urll = 'https://commons.wikimedia.org/wiki/File:'+image_name.replace(' ','_')+'?uselang='+language\n",
    "    #print(urll)\n",
    "        page = requests.get(urll)\n",
    "        if page.status_code == 404:\n",
    "            return None,None,None,None,None,None,None,None,None\n",
    "    except:\n",
    "            return None,None,None,None,None,None,None,None,None\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    #\n",
    "    #getting descriptions\n",
    "    desc = None\n",
    "    descriptions = []\n",
    "    descriptions=soup.find_all('div', class_='description mw-content-'+rtl+' '+language)\n",
    "    if len (descriptions)==0:\n",
    "        descriptions=soup.find_all(lambda tag: tag.name == ['td'] and \n",
    "                                tag.get('class') == ['description'] and tag.get('lang') == [language])\n",
    "        if len (descriptions)==0:\n",
    "            descriptions=soup.find_all(lambda tag: tag.name == ['div'] and \n",
    "                                    tag.get('class') == ['description'] and tag.get('lang') == [language])\n",
    "    if len (descriptions)==0 and language =='en': \n",
    "        descriptions=soup.find_all(lambda tag: tag.name == 'div' and \n",
    "                               tag.get('class') == ['description'])\n",
    "        if len (descriptions)==0:\n",
    "            descriptions=soup.find_all(lambda tag: tag.name == 'td' and \n",
    "                               tag.get('class') == ['description'])\n",
    "        if len(descriptions)>0 and len(descriptions[0].find_all(\"div\"))>0:\n",
    "            descriptions=[]\n",
    "    if len(descriptions)>0:\n",
    "        for description in descriptions:\n",
    "            desc = description.get_text().strip()\n",
    "#     for english (there are 3 different ways to get descriptions)\n",
    "    #\n",
    "    #getting captions\n",
    "    capt = None\n",
    "    try:\n",
    "        caption = soup.find_all('div', class_ = 'wbmi-caption-value', lang=language)[0]\n",
    "        if (not caption is None and not len(caption['class'])>1):\n",
    "                capt = caption.get_text()\n",
    "    except IndexError:\n",
    "        pass\n",
    "    #\n",
    "#     #getting categories\n",
    "#     categories = soup.find_all('div')\n",
    "#     for category in categories:\n",
    "#         c = category.find(href='/wiki/Special:Categories')\n",
    "#         c2 = category.find('div')\n",
    "#         if (c != None and c2 == None): #get the \"smallest\" div around the specific href\n",
    "#             cats= category.find_all(text=True)[2:]\n",
    "#             break\n",
    "#         else:\n",
    "#             cats=[]\n",
    "#     #\n",
    "#     #parsing image atrributes to get size, user who uploaded the image, and date\n",
    "#     size=soup.find('img', attrs={'data-file-width': True})\n",
    "#     size=str(size['data-file-height'])+'x'+str(size['data-file-width'])\n",
    "#     user=soup.find(class_='mw-userlink')\n",
    "#     user=user['title']\n",
    "#     date=soup.find('td',class_='filehistory-selected')\n",
    "#     date=date.get_text()\n",
    "#     #\n",
    "    #parsing the structured data json to get tags and copuright\n",
    "    depicts_statements = soup.find_all('div', attrs={'data-formatvalue': True,'data-property':['P180','P275']})\n",
    "    depicts=[]\n",
    "    copyright=None\n",
    "    for depiction in depicts_statements:\n",
    "            data=json.loads(depiction['data-formatvalue'])\n",
    "            if len(data)>0:\n",
    "                for d in list(data.values())[1:]:\n",
    "                    extracted=d['text/plain'][language]\n",
    "                    propname=(list(extracted.keys()))[0]\n",
    "                    if propname=='P180':\n",
    "                        #print(extracted)\n",
    "                        depicts.append(list(extracted.values())[0])\n",
    "                    elif propname=='P275':\n",
    "                        copyright=list(extracted.values())[0]\n",
    "    #\n",
    "    #parsing image extension\n",
    "    extension=image_name[image_name.rfind('.')+1:]\n",
    "    #return desc,capt,cats,size,user,depicts,date,copyright,extension\n",
    "    return desc,capt,depicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#properties={}\n",
    "for wiki in languages:\n",
    "    properties[wiki]=[]\n",
    "    language=wiki.replace('wiki','')\n",
    "    rtllan=rtls[wiki]\n",
    "    #this has to be substituted with the right input method/file extracting unillustrated articles and their corresponding potential candidates \n",
    "    data=pd.read_csv('Output/'+wiki+'_wd_image_candidates.tsv',sep='\\t')\n",
    "    data=data.drop('Unnamed: 0',axis=1)\n",
    "    #for the purpose of this proof of concept\n",
    "    data_small=data.sample(min(len(data),sample_size))\n",
    "    #downloading properties for each available image\n",
    "    print(language)\n",
    "    for imagelist in data_small['top_candidates']:\n",
    "        for imagedic in ast.literal_eval(imagelist):\n",
    "            image=imagedic['image']\n",
    "            if image is not None:\n",
    "                    properties[wiki][image]=get_properties(image,language,rtllan)\n",
    "            else:\n",
    "                    properties[wiki][image]=[None,None,None]#,None,None,None,None,None,None])\n",
    "    #updating tabl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['enwiki', 'arwiki', 'kowiki', 'cswiki', 'viwiki', 'frwiki', 'fawiki', 'ptwiki', 'ruwiki', 'trwiki', 'plwiki', 'hewiki', 'svwiki', 'ukwiki', 'huwiki', 'hywiki', 'srwiki', 'euwiki', 'arzwiki', 'cebwiki', 'dewiki', 'bnwiki'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_large.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en\n",
    "missing descriptions: 0.4769\n",
    "missing captions: 0.9638\n",
    "missing depicts: 0.9392\n",
    "ar\n",
    "missing descriptions: 0.9913\n",
    "missing captions: 0.9942\n",
    "missing depicts: 0.9326\n",
    "ko\n",
    "missing descriptions: 0.9903\n",
    "missing captions: 0.9984\n",
    "missing depicts: 0.9522\n",
    "cs\n",
    "missing descriptions: 0.9812\n",
    "missing captions: 0.9981\n",
    "missing depicts: 0.9425\n",
    "vi\n",
    "missing descriptions: 0.9997\n",
    "missing captions: 1.0\n",
    "missing depicts: 0.9372\n",
    "fr\n",
    "missing descriptions: 0.9384\n",
    "missing captions: 0.9925\n",
    "missing depicts: 0.9149\n",
    "fa\n",
    "missing descriptions: 0.9931\n",
    "missing captions: 0.9934\n",
    "missing depicts: 0.9565\n",
    "pt\n",
    "missing descriptions: 0.984\n",
    "missing captions: 0.9952\n",
    "missing depicts: 0.9555\n",
    "ru\n",
    "missing descriptions: 0.9577\n",
    "missing captions: 0.9925\n",
    "missing depicts: 0.9322\n",
    "tr\n",
    "missing descriptions: 0.9825\n",
    "missing captions: 0.9884\n",
    "missing depicts: 0.9564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage={'wiki':[],'descriptions':[],'captions':[],'depicts':[]}\n",
    "for wiki in languages:\n",
    "    coverage['wiki'].append(wiki)\n",
    "    for column in ['descriptions','captions','depicts']:#,'categories']:\n",
    "        coverage[column].append((data_small[wiki][column].isna().sum())/(len(data_small[wiki])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_large={'wiki':[],'descriptions':[],'captions':[],'depicts':[]}\n",
    "for wiki in languages:\n",
    "    coverage_large['wiki'].append(wiki)\n",
    "    for column in ['descriptions','captions','depicts']:#,'categories']:\n",
    "        coverage_large[column].append((data_large[wiki][column].isna().sum())/(len(data_large[wiki])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coveragepd=pd.DataFrame(coverage_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "coveragepd.to_csv('metadata_coverage_large.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print to file for user study\n",
    "data_small_clean=data_small[['item_id','page_id','page_title','selected','notes','descriptions','captions','categories','depicts','size','copyright','date','user','image type']]\n",
    "data_small_clean.to_csv('sample_suggestion_data_with_annotations_'+wiki+'.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.434"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total images for which we get candidates\n",
    "len(data_small)/sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
